{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras LSTM Sentiment Analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUnowXpgq6h6"
      },
      "source": [
        "!pip install validators"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfdDsbFvpaHD"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import twitter_samples,stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "import string\n",
        "import validators\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import re\n",
        "nltk.download('wordnet') \n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "ps = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSfMW0Raq3sA"
      },
      "source": [
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "tweets = all_positive_tweets + all_negative_tweets\n",
        "labels = np.append(np.ones((len(all_positive_tweets),1)), np.zeros((len(all_negative_tweets),1)), axis = 0)\n",
        "print(labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSTpAkC8q-nR"
      },
      "source": [
        "def Process_Tweet(tweet):\n",
        "  tweet = tweet.replace(\"n't\",\" not\").replace(\"'m\",\" am\").replace(\"'s\",\" is\").replace(\"'ve\",\" have\").replace(\"â€™\",\"'\").replace('`','')\n",
        "  tweet = tweet.split()\n",
        "  alphabets = \"qwertyuioplkjhgfdsazxcvbnm\"\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tweet = [w.lower() for w in tweet if w not in stop_words and w[0] not in string.punctuation and not validators.url(w) and w[0] in alphabets]\n",
        "  tweet = word_tokenize(\" \".join(tweet))\n",
        "  tweet = [\"<s>\"]+[re.sub(r'(.)\\1{2,}', r'\\1', w) for w in tweet if w not in string.punctuation] +[\"<\\s>\"]\n",
        "  tweet = nltk.pos_tag(tweet)\n",
        "  tweet = [(ps.lemmatize(w[0]),w[1]) for w in tweet]\n",
        "  return tweet"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcQ5PJXJrLk0"
      },
      "source": [
        "processsed_i = []\n",
        "X = []\n",
        "Y = []\n",
        "postive_list = []\n",
        "negative_list = []\n",
        "vocab = set()\n",
        "for i in range(len(tweets)):\n",
        "  processsed_i = Process_Tweet(tweets[i])\n",
        "  print(\"\\rTweets\",str(i+1),\":\",processsed_i,end=\"\")\n",
        "  if processsed_i != []:\n",
        "    vocab.update(processsed_i)\n",
        "    if labels[i]==1:\n",
        "      postive_list = postive_list + processsed_i\n",
        "      Y.append(1)\n",
        "    else:\n",
        "      Y.append(0)\n",
        "      negative_list = negative_list + processsed_i\n",
        "    X.append(processsed_i)\n",
        "print(\"\\nNumber of tweets:\", len(X),len(Y))\n",
        "vocab = list(vocab)\n",
        "print(\"\\nVocab Size:\",len(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_bVPLrPtYsV"
      },
      "source": [
        "positive_count = dict()\n",
        "negative_count = dict()\n",
        "for j in range(len(vocab)):\n",
        "  i = vocab[j]\n",
        "  print(\"\\r\",j,\"/\",len(vocab),end=\"\")\n",
        "  if postive_list.count(i)>0:\n",
        "    positive_count[i] = postive_list.count(i)\n",
        "  if negative_list.count(i)>0:\n",
        "    negative_count[i] = negative_list.count(i)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE06mKuuuwMV"
      },
      "source": [
        "Processed_X = []\n",
        "Processed_Y = []\n",
        "length = 15\n",
        "list_X = []\n",
        "actual_X = []\n",
        "for x_val in range(len(X)):\n",
        "  i = X[x_val]\n",
        "  k = 0\n",
        "  pos_list = [0]*length\n",
        "  neg_list = [0]*length\n",
        "  if len(i)<length:\n",
        "    for x in range(1,len(i)-1):\n",
        "        j = i[x]\n",
        "        if j[1] == 'NNP':\n",
        "          continue\n",
        "        if j in positive_count.keys():\n",
        "            pos_list[k] += positive_count[j]\n",
        "        if j in negative_count.keys():\n",
        "            neg_list[k] += negative_count[j]\n",
        "        k+=1\n",
        "    list_X.append(X[x_val])\n",
        "    actual_X.append(tweets[x_val])\n",
        "    Processed_X.append([pos_list,neg_list])\n",
        "    Processed_Y.append(Y[x_val])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb_3qfDSzqgP"
      },
      "source": [
        "X_array = np.array(Processed_X)#.reshape((len(Processed_X),length,2))\n",
        "X_array = np.transpose(X_array,axes=(0,2,1))\n",
        "X_array.shape\n",
        "val = 19\n",
        "print(\"Actual Tweet:\",actual_X[val])\n",
        "print(\"After preprocessing:\",list_X[val])\n",
        "print(\"Representation:\\n\",X_array[val])\n",
        "Y_array = np.array(Processed_Y).reshape((len(Processed_Y),1))\n",
        "print(\"Label:\",Y_array[val])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvQ9oirI0Gws"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM, Reshape\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eMKOKwH0aGH"
      },
      "source": [
        "def create_model(inp_shape,batch_size):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(512, input_shape = inp_shape, activation = 'relu'))\n",
        "    model.add(Dense(1024,activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(2048,activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(2048,activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(1,activation='sigmoid'))\n",
        "    return model"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM9sTqqp0cyw"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "model = create_model(X_array[0].shape,BATCH_SIZE)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXN3j65m0eTC"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_array,Y_array,train_size=0.8, random_state=7)\n",
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hha6VCu90iFe"
      },
      "source": [
        "opt = Adam(learning_rate=0.00005)\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(x=x_train,y=y_train,epochs=35,validation_split=0.1,batch_size=BATCH_SIZE,validation_steps=20,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyL92iKU0pVf"
      },
      "source": [
        "history = model.fit(x=x_train,y=y_train,epochs=35,validation_split=0.1,batch_size=BATCH_SIZE,validation_steps=20,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7oYeD8V0wjf"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'Val'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'Val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwfpUkGhN0q1"
      },
      "source": [
        "Y_pred = model.predict(x_test)\n",
        "print(Y_pred)\n",
        "print(Y_pred.shape)\n",
        "Y_pred = np.round(Y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl3LRfD2Yr6E"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, Y_pred, labels=[0,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt_D7BtWYwmM"
      },
      "source": [
        "acc = (cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])\n",
        "print(acc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}